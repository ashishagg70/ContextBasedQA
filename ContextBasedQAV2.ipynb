{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ContextBasedQAV2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishagg70/ContextBasedQA/blob/master/ContextBasedQAV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kekUt8v-quR",
        "outputId": "48c89a23-bfa6-483b-a7e1-1dd49cdb269c"
      },
      "source": [
        "!wget \"https://data.deepai.org/squad1.1.zip\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-20 06:19:38--  https://data.deepai.org/squad1.1.zip\n",
            "Resolving data.deepai.org (data.deepai.org)... 138.201.36.183\n",
            "Connecting to data.deepai.org (data.deepai.org)|138.201.36.183|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9152254 (8.7M) [application/x-zip-compressed]\n",
            "Saving to: ‘squad1.1.zip’\n",
            "\n",
            "squad1.1.zip        100%[===================>]   8.73M  42.5MB/s    in 0.2s    \n",
            "\n",
            "2021-04-20 06:19:38 (42.5 MB/s) - ‘squad1.1.zip’ saved [9152254/9152254]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrRUlX-PAweX",
        "outputId": "735888b9-39b8-4b67-9fc2-2885146e8bf0"
      },
      "source": [
        "!unzip squad1.1.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  squad1.1.zip\n",
            "  inflating: dev-v1.1.json           \n",
            "  inflating: train-v1.1.json         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c3BWjc-B4FC",
        "outputId": "dee4b464-ede8-4dde-f1af-89de632bda1a"
      },
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import word_tokenize\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchtext.vocab import GloVe\n",
        "import json\n",
        "import tqdm\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "if device == \"cuda\":\n",
        "    num_workers = 1\n",
        "    pin_memory = True\n",
        "else:\n",
        "    num_workers = 0\n",
        "    pin_memory = False"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyqvIyLTBw37"
      },
      "source": [
        "file = open('train-v1.1.json','r')\n",
        "file2 = open('dev-v1.1.json','r')\n",
        "train_data = json.load(file)['data']\n",
        "dev_data = json.load(file2)['data']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ8l5Bw91057"
      },
      "source": [
        "MAX_WORD_LENGTH = 40\n",
        "D = 100"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPdx76dabgJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef62712e-f718-4a5e-fc92-acc945f839da"
      },
      "source": [
        "glove = GloVe('840B',300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.840B.300d.zip:  35%|███▍      | 753M/2.18G [02:33<07:04, 3.35MB/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D8lLh6QOrNJ"
      },
      "source": [
        "char_to_index = defaultdict(int)\n",
        "index_to_char = defaultdict(str)\n",
        "# word_to_index = defaultdict(int)\n",
        "# index_to_word = defaultdict(str)\n",
        "\n",
        "num_contexts = 0\n",
        "num_questions = 0\n",
        "num_characters = 0\n",
        "num_words = len(glove)\n",
        "\n",
        "char_contexts = []\n",
        "char_questions = []\n",
        "\n",
        "word_contexts=[]\n",
        "word_questions = []\n",
        "answers = []\n",
        "\n",
        "input_data = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D4QaCfRCxCv"
      },
      "source": [
        "# i=1\n",
        "# for key in glove.itos:\n",
        "#     word_to_index[key]=i\n",
        "#     index_to_word[i]=key\n",
        "#     i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcyBJLwIGEXD"
      },
      "source": [
        "def char_encode(text, is_train=True):\n",
        "    global char_to_index\n",
        "    global index_to_char\n",
        "    global num_characters\n",
        "\n",
        "    text_endcoding = []\n",
        "    for word in text:\n",
        "        encoding = np.zeros(MAX_WORD_LENGTH)\n",
        "        try:\n",
        "            i=0\n",
        "            for char in word:\n",
        "                encode = char_to_index[char]\n",
        "                if is_train == True and encode == 0:\n",
        "                    index = num_characters+1\n",
        "                    char_to_index[char]= index\n",
        "                    index_to_char[index]= char\n",
        "                    encode = index\n",
        "                    num_characters+=1\n",
        "                encoding[i]=encode\n",
        "                i+=1\n",
        "        except:\n",
        "            print(word)\n",
        "        text_endcoding.append(encoding)\n",
        "    return text_endcoding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4gs1MmVXaJm"
      },
      "source": [
        "def convert_to_lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "def perform_word_tokenization(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "def word_encode(text, is_train=True):\n",
        "    global word_to_index\n",
        "    global index_to_word\n",
        "    global num_words\n",
        "    encoding =[]\n",
        "    for word in text:\n",
        "        try:\n",
        "            encode = glove.stoi[word]\n",
        "        except:\n",
        "            encode = num_words\n",
        "        encoding.append(encode)\n",
        "    return encoding\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D2yQ_1Hpo9E"
      },
      "source": [
        "def text_preprocess(text, is_train=True):\n",
        "    text = convert_to_lower(text)\n",
        "    text = perform_word_tokenization(text)\n",
        "    word_encoding = word_encode(text)\n",
        "    char_encoding = char_encode(text, is_train)\n",
        "    return char_encoding, word_encoding\n",
        "\n",
        "def get_answer_indices(context,start,answer):\n",
        "    context=convert_to_lower(context)\n",
        "    tokens = perform_word_tokenization(context[:(start+1)])\n",
        "    ans_tokens = perform_word_tokenization(convert_to_lower(answer))\n",
        "    start = len(tokens)-1\n",
        "    return start, start + len(ans_tokens)-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWkU59lnXcvm"
      },
      "source": [
        "for obj in train_data:\n",
        "    for para in obj['paragraphs']:\n",
        "        cce, cwe = text_preprocess(para['context'])\n",
        "        char_contexts.append(cce)\n",
        "        word_contexts.append(cwe)\n",
        "        num_contexts+=1\n",
        "        for qa in para['qas']:\n",
        "            qce, qwe = text_preprocess(qa['question'])\n",
        "            char_questions.append(qce)\n",
        "            word_questions.append(qwe)\n",
        "            num_questions+=1\n",
        "            input_data.append((num_contexts-1,num_questions-1))\n",
        "            ans = []\n",
        "            for a in qa['answers']:\n",
        "                start,end = get_answer_indices(para['context'],a['answer_start'],a['text'])\n",
        "                ans.append((start,end))\n",
        "            answers.append(ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQQTawJsyIuH"
      },
      "source": [
        "char_vocab_size = len(char_to_index)+1\n",
        "word_vocab_size = num_words+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wmu2m_5ovDt1"
      },
      "source": [
        "class CharEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self,vocab_size,embedding_dim = 8, cnn_kernel_size = 5,word_embedding_size =100 ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size,embedding_dim=embedding_dim,padding_idx=0,)\n",
        "        self.cnn = nn.Conv1d(in_channels=embedding_dim,out_channels=word_embedding_size,kernel_size=cnn_kernel_size)\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size = MAX_WORD_LENGTH-cnn_kernel_size+1)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        # print(\"start\",x.shape)\n",
        "        batch_size = x.shape[0]\n",
        "        x = x.view(-1,MAX_WORD_LENGTH)\n",
        "        x = self.embedding(x)\n",
        "        x = x.transpose(1,2)\n",
        "        x = self.cnn(x)\n",
        "        x = F.relu(x)\n",
        "        # print(\"cnn\",x.shape)\n",
        "        x = self.maxpool(x)\n",
        "        # print(\"pool\",x.shape)\n",
        "        #TODO : ReLU\n",
        "        x = x.view(batch_size,-1,x.shape[1])\n",
        "        # print(\"final\",x.shape)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpPWBZflPsme"
      },
      "source": [
        "class WordEmbedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()      \n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.cat((glove.vectors,torch.zeros(1,glove.dim)),dim=0))\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.embedding(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uRRMe8Wuuk6"
      },
      "source": [
        "class HighwayNetworkLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.transform = nn.Sequential(\n",
        "               nn.Linear(glove.dim+D,glove.dim+D),\n",
        "               nn.ReLU(),\n",
        "               nn.Linear(glove.dim+D,glove.dim+D),\n",
        "               nn.ReLU()\n",
        "        )\n",
        "        self.gate = nn.Sequential(\n",
        "               nn.Linear(glove.dim+D,glove.dim+D),\n",
        "               nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        x_transformed = self.transform(x)\n",
        "        p = self.gate(x)\n",
        "        return p*x_transformed + (1-p)*x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFiEedGLXNUQ"
      },
      "source": [
        "class ContextualEmbedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.bilstm = nn.LSTM(glove.dim+D,D,1,bidirectional = True,batch_first = True)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x,_ = self.bilstm(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udU65UyxlAoZ"
      },
      "source": [
        "class AttentionFlowLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.alpha = nn.Linear(6*D,1)\n",
        "    \n",
        "    def forward(self,H,U):\n",
        "        # H : contextual embedding of context\n",
        "        # U : contextual embedding of query\n",
        "\n",
        "        T = H.shape[1]\n",
        "        J = U.shape[1]\n",
        "\n",
        "        H_interleaved = torch.repeat_interleave(H,J,dim=1)\n",
        "        U_repeated = U.repeat(1,T,1)\n",
        "\n",
        "        assert(H_interleaved.shape==U_repeated.shape)\n",
        "\n",
        "        HU = torch.cat((H_interleaved,U_repeated,H_interleaved*U_repeated),dim=-1)\n",
        "        S = self.alpha(HU)\n",
        "        S = S.view(-1,T,J)\n",
        "        C2Q_att = F.softmax(S,dim = -1)\n",
        "        U_tilde = torch.matmul(C2Q_att,U)\n",
        "        Q2C_att = F.softmax(torch.max(S,dim=-1)[0],dim=-1)\n",
        "        Q2C_att = Q2C_att.unsqueeze(1)\n",
        "        H_tilde = torch.matmul(Q2C_att,H).repeat(1,T,1)\n",
        "        G = torch.cat((H,U_tilde,H*U_tilde,H*H_tilde),dim=-1)\n",
        "        return G\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2UgY5lb7WIg"
      },
      "source": [
        "class ModellingLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.bilstm = nn.LSTM(8*D,D,2,bidirectional = True,batch_first = True)\n",
        "    def forward(self,G):\n",
        "        M,_ = self.bilstm(G)\n",
        "        return M"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3SaUZ93_Zsm"
      },
      "source": [
        "class OutputLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dense1 = nn.Linear(10*D,1)\n",
        "        self.dense2 = nn.Linear(10*D,1)\n",
        "        self.bilstm = nn.LSTM(2*D,D,1,bidirectional = True,batch_first = True)\n",
        "        self.softmax = nn.LogSoftmax(dim=-1)\n",
        "    def forward(self,G,M):\n",
        "        GM = torch.cat((G,M),dim=-1)\n",
        "        temp_GM = self.dense1(GM).squeeze(-1)\n",
        "        start = self.softmax(temp_GM)\n",
        "        M2,_ = self.bilstm(M)\n",
        "        GM2= torch.cat((G,M2),dim=-1)\n",
        "        temp_GM2 = self.dense2(GM2).squeeze(-1)\n",
        "        end = self.softmax(temp_GM2)\n",
        "\n",
        "        return start,end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68KnieuIHCuG"
      },
      "source": [
        "class BiDAF(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.char_emb_layer=CharEmbedding(char_vocab_size,embedding_dim=8)\n",
        "        self.word_emb_layer=WordEmbedding()\n",
        "        self.highway = HighwayNetworkLayer()\n",
        "        self.cont_emb_layer=ContextualEmbedding()\n",
        "        self.att_layer = AttentionFlowLayer()\n",
        "        self.modelling_layer = ModellingLayer()\n",
        "        self.output_layer = OutputLayer()\n",
        "    \n",
        "    def forward(self,context_char,context_word,query_char,query_word):\n",
        "        context_char_emb = self.char_emb_layer(context_char)\n",
        "        context_word_emb = self.word_emb_layer(context_word)\n",
        "        final_context_word_embedding = torch.cat((context_char_emb,context_word_emb),dim = -1)\n",
        "        final_context_word_embedding = self.highway(final_context_word_embedding)\n",
        "        context_cont_emb = self.cont_emb_layer(final_context_word_embedding)\n",
        "\n",
        "        query_char_emb = self.char_emb_layer(query_char)\n",
        "        query_word_emb = self.word_emb_layer(query_word)\n",
        "        final_query_word_embedding = torch.cat((query_char_emb,query_word_emb),dim = -1)\n",
        "        final_query_word_embedding = self.highway(final_query_word_embedding)\n",
        "        query_cont_emb = self.cont_emb_layer(final_query_word_embedding)\n",
        "\n",
        "        g = self.att_layer(context_cont_emb,query_cont_emb)\n",
        "        m = self.modelling_layer(g)\n",
        "        o = self.output_layer(g,m)\n",
        "        return o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0dV56dIzt58"
      },
      "source": [
        "# char_batch_context = [char_contexts[0],char_contexts[0]]\n",
        "# char_batch_query = [char_questions[0],char_questions[1]]\n",
        "# word_batch_context=[word_contexts[0],word_contexts[0]]\n",
        "# word_batch_query = [word_questions[0],word_questions[1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utt6W-Oi3mBx"
      },
      "source": [
        "# char_batch_context_t = torch.LongTensor(nn.utils.rnn.pad_sequence([torch.LongTensor(sent) for sent in char_batch_context],batch_first=True))\n",
        "# char_batch_query_t = torch.LongTensor(nn.utils.rnn.pad_sequence([torch.LongTensor(sent) for sent in char_batch_query],batch_first=True))\n",
        "# word_batch_context_t = torch.LongTensor(nn.utils.rnn.pad_sequence([torch.LongTensor(sent) for sent in word_batch_context],batch_first=True))\n",
        "# word_batch_query_t = torch.LongTensor(nn.utils.rnn.pad_sequence([torch.LongTensor(sent) for sent in word_batch_query],batch_first=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5nPLT92Mc0g"
      },
      "source": [
        "def collate_fn(idx):\n",
        "    context_char,context_word,query_char,query_word,ans_start, ans_end=[],[],[],[],[],[]\n",
        "    for id in idx:\n",
        "        c,q = input_data[id]\n",
        "        context_char.append(torch.LongTensor(char_contexts[c]))\n",
        "        context_word.append(torch.LongTensor(word_contexts[c]))\n",
        "        query_char.append(torch.LongTensor(char_questions[q]))\n",
        "        query_word.append(torch.LongTensor(word_questions[q]))\n",
        "        start,end = answers[q][0]\n",
        "        ans_start.append(start)\n",
        "        ans_end.append(end)\n",
        "    \n",
        "    context_char = torch.LongTensor(nn.utils.rnn.pad_sequence(context_char,batch_first=True))\n",
        "    context_word = torch.LongTensor(nn.utils.rnn.pad_sequence(context_word,batch_first=True))\n",
        "    query_char = torch.LongTensor(nn.utils.rnn.pad_sequence(query_char,batch_first=True))\n",
        "    query_word = torch.LongTensor(nn.utils.rnn.pad_sequence(query_word,batch_first=True))\n",
        "    ans_start = torch.LongTensor(ans_start)\n",
        "    ans_end = torch.LongTensor(ans_end)\n",
        "\n",
        "    return context_char,context_word,query_char,query_word,ans_start,ans_end,idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rs9pZYyME81"
      },
      "source": [
        "batch_size = 32\n",
        "# num_workers = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqPnfiXR8G39"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    range(len(input_data)),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    # num_workers=num_workers,\n",
        "    # pin_memory=pin_memory\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4ZFT7uoK-2m"
      },
      "source": [
        "bidaf = BiDAF().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-563BdfKhhb"
      },
      "source": [
        "epochs = 12\n",
        "\n",
        "learning_rate = 0.5\n",
        "\n",
        "print_every = 50\n",
        "\n",
        "optimizer = torch.optim.Adadelta(bidaf.parameters(),lr=learning_rate)\n",
        "\n",
        "criterion = nn.NLLLoss()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvXrLr4qLHCo"
      },
      "source": [
        "bidaf.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for batch_idx, _data in enumerate(tqdm.notebook.tqdm(iter(train_loader))):\n",
        "        context_char,context_word,query_char,query_word,ans_start,ans_end,idx = _data\n",
        "        context_char,context_word,query_char,query_word,ans_start,ans_end = context_char.to(device),context_word.to(device),query_char.to(device),query_word.to(device),ans_start.to(device),ans_end.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        start,end = bidaf(context_char,context_word,query_char,query_word)\n",
        "        loss1 = criterion(start,ans_start)\n",
        "        loss2 = criterion(end,ans_end)\n",
        "        loss = loss1+loss2\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            if batch_idx % print_every == 0 :\n",
        "                print(\"epoch = {},batch = {},loss={},loss_1={},loss_2={}\".format(epoch,batch_idx,loss.item(),loss1.item(),loss2.item()))\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAUSnlZC8Mq3"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc_5pohcASTG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}